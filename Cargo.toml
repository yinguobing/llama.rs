[package]
name = "llama3"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
candle-flash-attn = { git = "https://github.com/huggingface/candle.git", version = "0.5.0", optional = true }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
cudarc = { version = "0.10.0", features = ["f16"] }
serde_json = "1.0.116"
tokenizers = { version = "0.15.0", default-features = false, features = [
    "onig",
] }

[features]
default = ["cuda"]
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
cudnn = ["candle-core/cudnn", "candle-nn/cuda", "candle-transformers/cuda"]
flash-attn = ["dep:candle-flash-attn", "candle-transformers/flash-attn", "cuda"]
