[package]
name = "llama3"
version = "0.1.0"
edition = "2021"
description = "Running LLaMA 3"
license = "MIT OR Apache-2.0"
authors = ["尹国冰", "yinguobing <yinguobing@gmail.com>"]
repository = "https://github.com/yinguobing/llama.rs"
keywords = ["LLM", "LLaMA", "AI"]
readme = "README.md"

[dependencies]
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
candle-flash-attn = { git = "https://github.com/huggingface/candle.git", version = "0.5.0", optional = true }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.5.0" }
cudarc = { version = "0.10.0", features = ["f16"] }
serde = { version = "1.0.199", features = ["serde_derive"] }
serde_json = "1.0.116"
thiserror = "1.0.59"
tokenizers = { version = "0.15.0", default-features = false, features = [
    "onig",
] }

[features]
default = ["cuda"]
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
cudnn = ["candle-core/cudnn", "candle-nn/cuda", "candle-transformers/cuda"]
flash-attn = ["dep:candle-flash-attn", "candle-transformers/flash-attn", "cuda"]
